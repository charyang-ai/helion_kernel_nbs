{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9ed5955",
   "metadata": {},
   "source": [
    "# Helion Kernel Development & Benchmark with Triton, Torch\n",
    "\n",
    "[Helion](https://github.com/pytorch/helion) is a Python-embedded domain-specific language (DSL) for authoring machine learning kernels, designed to compile down to Triton, a performant backend for programming GPUs and other devices. Helion aims to raise the level of abstraction compared to Triton, making it easier to write correct and efficient kernels while enabling more automation in the autotuning process.\n",
    "\n",
    "Helion can be viewed either as PyTorch with tiles or as a higher-level Triton. Compared to Triton, Helion reduces manual coding effort through autotuning. Helion spends more time (approx 10 min) autotuning as it evaluates hundreds of potential Triton implementations generated from a single Helion kernel. This larger search space also makes kernels more performance portable between different hardware. \n",
    "\n",
    "Helion has been supported by AMD GPUs. This workshop will demonstrate how to set up the Helion development environment and develop Helion Kernel, and benchmark performance with Triton and Torch on AMD MI GPU. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Agenda:\n",
    "- 1. Set up Helion Development Enviroment \n",
    "\n",
    "- 2. The Origin of Softmax Algorithm\n",
    "\n",
    "- 3. Helion Softmax GPU Kernel Authoring\n",
    "\n",
    "- 4. Helion VS. Triton VS. Torch Performance Benchmark & Visualization on AMD GPU\n",
    "\n",
    "\n",
    "## 1. Set up Helion development enviroment \n",
    " \n",
    "\n",
    "### Step1: Access AMD MI GPU \n",
    "In this workshop, we will use the **MI GPU** cloud instance from AMD Developer Cloud. Please access your GPU instance link, which has been provided by this workshop.\n",
    "\n",
    "### Step2: Run ROCm Official Container on GPU card \n",
    "In this workshop, we will work on the pre-built ROCm PyTorch image, which pulled with command 'docker pull rocm/pytorch:rocm7.0.2_ubuntu24.04_py3.12_pytorch_release_2.8.0'. It has integrated Pytorch with AMD ROCm software stack succeffully. Developers can also try other ROCm as the base image docker hub page from [docker images](https://hub.docker.com/r/rocm/pytorch/tags) if need. \n",
    "\n",
    "### Step3: Install OpenAI Helion and Triton\n",
    "\n",
    "####    Uninstall the old Helion & Triton\n",
    "It is strongly recommended to use the latest version Helion in your project, because AMD and other vendors are updating their optimization passes and algorithms frequently in [Meta Helion](https://github.com/pytorch/helion) , which can help improve your Helion kernel performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fea56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y helion triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c3edf",
   "metadata": {},
   "source": [
    "#### Install Meta PyTorch, Meta Helion and OpenAI Triton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# if not docker based, pls uncomment below line:\n",
    "# pip install torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0 --index-url https://download.pytorch.org/whl/rocm6.4\n",
    "pip install triton\n",
    "pip install helion\n",
    "pip install matplotlib\n",
    "\n",
    "pip list | grep -E 'helion|triton|torch'\n",
    "\n",
    "# Pls ignore the incompatible error which will not  affect the examples' exection in this notebook.\n",
    "# Confirm it install successfully by showing 'Successfully installed triton-xxx'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf4b5b9",
   "metadata": {},
   "source": [
    "## 2. The Origin of Softmax Algorithm\n",
    "\n",
    "The softmax function, often used in classification CNN models and even Transformer based LLM models,converts raw output scores or logits, into probabilities by taking the exponential of each value and normalizing these values by dividing by the sum of all the exponentials. This process ensures that the output values are in the range (0,1) and sum up to 1, making them interpretable as probabilities. PyTorch has implemented it as [a standard API](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html). \n",
    "\n",
    "Definition of function $y = Softmax(x)$ is:\n",
    "\n",
    "$$\n",
    "y_i = \\frac{e^{x_i}}{\\sum_{j=1}^{V} e^{x_j}} \\tag{1}\n",
    "$$\n",
    "\n",
    "where $x,y \\in \\mathbb{R}^V$.\n",
    "\n",
    "### Naive version - Safe-Softmax\n",
    "\n",
    "To achieve the numerical stability, we need to subtract the maximum value of the row vector from each input element before taking their exponentials. So the definition changes to:\n",
    "\n",
    "$$\n",
    "y_i = \n",
    "\\frac{e^{\\left(x_i - \\max_{k=1}^V x_k\\right)}}\n",
    "     {\\sum_{j=1}^V e^{\\left(x_j - \\max_{k=1}^V x_k\\right)}} \\tag{2}\n",
    "$$\n",
    "\n",
    "where $x,y \\in \\mathbb{R}^V$. This is called `Safe Softmax`.\n",
    "\n",
    "According to the softmax algorithm definition `(Equation 2)`, we implemented the naive version Triton kernel. To get the maximum data and the corresponding sum of all the exponentials, 2 for-loops are implemented in this version kernel, and there is still 1 for-loop to calculate the final softmax result. So total 3 loops are used in this kernel. The algorithm of `Safe Softmax` (from [this paper](https://arxiv.org/pdf/1805.02867)) is:\n",
    "\n",
    "![safe_softmax](./assets/safe_softmax_algo.png)\n",
    "\n",
    "Assuming that we need to test this kernel performance on an 8192x8192 tensor. We will conduct the calculation like this:\n",
    "\n",
    "![softmax_naive](./assets/softmax_naive_50p.png)\n",
    "\n",
    "- The block size of col dimension is 256.\n",
    "- We allocate one program per row of the input tensor, which means the grid size is (n_rows,) where n_rows equals number of rows of input tensor.\n",
    "- The program instance (thread block) scans one row of the tensor and iteratively process the data blocks of current row, to calculate the maximum value $m_k$ of current row. This is 1st for-loop.\n",
    "- The program instance (thread block) scans one row of the tensor and iteratively process the data blocks of current row, to calculate the denominator (sum of exponentials) value $d_j$ of current row. This is 2nd for-loop.\n",
    "- The program instance (thread block) scans one row of the tensor and iteratively process the data blocks of current row, to calculate the final softmax value $y_i$ of current row. This is 3rd for-loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc218853-c7e7-4cda-b9cc-11b060380a84",
   "metadata": {},
   "source": [
    "## 3. Helion Two-pass Softmax \n",
    "This example demonstrates multiple Helion kernel implementations of the softmax function,\n",
    "including a simple wrapper around PyTorch's softmax, and a numerically optimized two-pass version.\n",
    "The example also includes a check function to compare these kernels against PyTorch's\n",
    "built-in softmax for correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beeb2c5-8ca1-4377-8885-9d137c44e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import helion\n",
    "from helion._testing import run_example\n",
    "import helion.language as hl\n",
    "\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# %%\n",
    "@helion.kernel(autotune_effort=\"quick\")\n",
    "def softmax_two_pass(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Numerically optimized Helion kernel performing softmax in two passes.\n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor of shape [m, n].\n",
    "    Returns:\n",
    "        torch.Tensor: Softmax output tensor of the same shape.\n",
    "    \"\"\"\n",
    "    m, n = x.size()\n",
    "    out = torch.empty_like(x)\n",
    "    block_size_m = hl.register_block_size(m)\n",
    "    block_size_n = hl.register_block_size(n)\n",
    "    for tile_m in hl.tile(m, block_size=block_size_m):\n",
    "        mi = hl.full([tile_m], float(\"-inf\"), dtype=torch.float32)\n",
    "        di = hl.zeros([tile_m], dtype=torch.float32)\n",
    "        for tile_n in hl.tile(n, block_size=block_size_n):\n",
    "            values = x[tile_m, tile_n]\n",
    "            local_amax = torch.amax(values, dim=1)\n",
    "            mi_next = torch.maximum(mi, local_amax)\n",
    "            di = di * torch.exp(mi - mi_next) + torch.exp(\n",
    "                values - mi_next[:, None]\n",
    "            ).sum(dim=1)\n",
    "            mi = mi_next\n",
    "        for tile_n in hl.tile(n, block_size=block_size_n):\n",
    "            values = x[tile_m, tile_n]\n",
    "            out[tile_m, tile_n] = torch.exp(values - mi[:, None]) / di[:, None]\n",
    "    return out\n",
    "\n",
    "\n",
    "# %%\n",
    "def check(m: int, n: int) -> None:\n",
    "    \"\"\"\n",
    "    Runs correctness checks comparing Helion softmax kernels against PyTorch's softmax.\n",
    "    Args:\n",
    "        m (int): Number of rows in input tensor.\n",
    "        n (int): Number of columns in input tensor.\n",
    "    \"\"\"\n",
    "    x = torch.randn([m, n], device=\"cuda\", dtype=torch.float16)\n",
    "    run_example(softmax_two_pass, lambda x: torch.nn.functional.softmax(x, dim=1), (x,))\n",
    "\n",
    "\n",
    "# %%\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main function to run the softmax kernel correctness check with example input size.\n",
    "    \"\"\"\n",
    "    check(4096, 2560)\n",
    "\n",
    "\n",
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7db3c6-f74a-46a2-98a0-7c5f0170f937",
   "metadata": {},
   "source": [
    "## 4. Performance Benchmark & Visualization: Helion vs. Triton vs. PyTorch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017e0ce9-5bc7-4cbe-b1aa-333093deb048",
   "metadata": {},
   "source": [
    "### Control Group: Triton Fused-Softmax \n",
    "This example demonstrates how to implement a fused softmax kernel using Triton, with architectural awareness for AMD ROCm/CDNA backends.\n",
    "\n",
    "OpenAI Triton provided a reference softmax sample codes with the name of \"fused-softmax\". Based on online softmax, it continued to simplify the maxmumim data calculation, which can remove 1 for-loop. it also ask the compiler to use more threads per row by increasing the number of warps, which is often tuned for better performance. and finally it improved the kernel lauching scheme by the GPU hardware properties, which can have the higher GPU kernel occupancy and better performance.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8684a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import triton\n",
    "import random\n",
    "import triton.language as tl\n",
    "from triton.runtime import driver\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def is_hip():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"hip\"\n",
    "\n",
    "\n",
    "def is_cdna():\n",
    "    return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',\n",
    "                                                                                   'gfx90a', 'gfx908', 'gfx950')\n",
    "\n",
    "@triton.jit\n",
    "def fused_softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,\n",
    "                   num_stages: tl.constexpr):\n",
    "    # starting row of the program\n",
    "    row_start = tl.program_id(0)\n",
    "    row_step = tl.num_programs(0)\n",
    "    for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):\n",
    "        # The stride represents how much we need to increase the pointer to advance 1 row\n",
    "        row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "        # The block size is the next power of two greater than n_cols, so we can fit each\n",
    "        # row in a single block\n",
    "        col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "        input_ptrs = row_start_ptr + col_offsets\n",
    "        # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n",
    "        mask = col_offsets < n_cols\n",
    "        row = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n",
    "        # Subtract maximum for numerical stability\n",
    "        row_minus_max = row - tl.max(row, axis=0)\n",
    "        # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n",
    "        numerator = tl.exp(row_minus_max)\n",
    "        denominator = tl.sum(numerator, axis=0)\n",
    "        softmax_output = numerator / denominator\n",
    "        # Write back output to DRAM\n",
    "        output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
    "        output_ptrs = output_row_start_ptr + col_offsets\n",
    "        tl.store(output_ptrs, softmax_output, mask=mask)\n",
    "\n",
    "# To tune the kernel, we first get some resource properties of our GPU by:\n",
    "properties = driver.active.utils.get_device_properties(DEVICE.index)\n",
    "NUM_SM = properties[\"multiprocessor_count\"]\n",
    "NUM_REGS = properties[\"max_num_regs\"]\n",
    "SIZE_SMEM = properties[\"max_shared_mem\"]\n",
    "WARP_SIZE = properties[\"warpSize\"]\n",
    "target = triton.runtime.driver.active.get_current_target()\n",
    "kernels = {}\n",
    "print(f\"NUM_SM: {NUM_SM}, NUM_REGS: {NUM_REGS}, SIZE_SMEM: {SIZE_SMEM}, WARP_SIZE: {WARP_SIZE}, target: {target}\")\n",
    "\n",
    "# Then we setup the kernel launch configuration\n",
    "torch.manual_seed(0)\n",
    "x = torch.randn(8192, 8192, device=DEVICE)\n",
    "output_torch = torch.softmax(x, dim=-1)\n",
    "n_rows, n_cols = x.shape\n",
    "# Allocate output\n",
    "y = torch.empty_like(x)\n",
    "\n",
    "# The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`\n",
    "BLOCK_SIZE = triton.next_power_of_2(n_cols*2)\n",
    "\n",
    "# Another trick we can use is to ask the compiler to use more threads per row by\n",
    "# increasing the number of warps (`num_warps`) over which each row is distributed.\n",
    "num_warps = 8\n",
    "\n",
    "# Number of software pipelining stages.\n",
    "num_stages = 4 if SIZE_SMEM > 200000 else 2\n",
    "\n",
    "print(f\"BLOCK_SIZE: {BLOCK_SIZE}, num_warps: {num_warps}, num_stages: {num_stages}\")\n",
    "\n",
    "# The occupancy of the kernel is limited by register usage. To maximize the occupancy, let's warmup the kernel to get register usage, and calculate the proper programs number.\n",
    "\n",
    "# pre-compile kernel to get register usage and compute thread occupancy.\n",
    "kernel = fused_softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,\n",
    "                                   num_stages=num_stages, num_warps=num_warps, grid=(1, ))\n",
    "kernel._init_handles()\n",
    "n_regs = kernel.n_regs\n",
    "size_smem = kernel.metadata.shared\n",
    "\n",
    "if is_hip():\n",
    "    if is_cdna():\n",
    "        NUM_GPRS = NUM_REGS * 2\n",
    "    MAX_NUM_THREADS = properties[\"max_threads_per_sm\"]\n",
    "    max_num_waves = MAX_NUM_THREADS // WARP_SIZE\n",
    "    occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps\n",
    "else:\n",
    "    occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)\n",
    "    \n",
    "occupancy = min(occupancy, SIZE_SMEM // size_smem)\n",
    "num_programs = NUM_SM * occupancy\n",
    "\n",
    "num_programs = min(num_programs, n_rows)\n",
    "\n",
    "print(f\"n_regs: {n_regs}, size_smem: {size_smem}, occupancy: {occupancy}, num_programs: {num_programs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311be852-0deb-444a-b5e4-083d168e617d",
   "metadata": {},
   "source": [
    "\n",
    "# Let's run the benchmark and visualization !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c763c6e-51e6-4e17-b46e-9ba0dd653e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import triton\n",
    "import random\n",
    "import triton.language as tl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# --- Helion Softmax ---\n",
    "def softmax_helion(x: torch.Tensor, dim=-1):\n",
    "    helion_output=softmax_two_pass(x)\n",
    "    return helion_output\n",
    "\n",
    "# --- Helper to run Triton Autotune ---\n",
    "def softmax_triton(x: torch.Tensor):\n",
    "    n_rows, n_cols = x.shape\n",
    "    triton_output = torch.empty_like(x)\n",
    "    kernel[(num_programs, 1, 1)](\n",
    "        y, \n",
    "        x, \n",
    "        x.stride(0), \n",
    "        y.stride(0), \n",
    "        n_rows, \n",
    "        n_cols, \n",
    "        BLOCK_SIZE, \n",
    "        num_stages)\n",
    "    return triton_output\n",
    "\n",
    "# --- PyTorch Naive Softmax ---\n",
    "def softmax_torch(x: torch.Tensor, dim=-1):\n",
    "    \"\"\"\n",
    "    Compute softmax using PyTorch built-in function.\n",
    "    Output is the same shape as input.\n",
    "    \"\"\"\n",
    "    torch_output = F.softmax(x, dim=dim)\n",
    "    return torch_output\n",
    "\n",
    "\n",
    "\n",
    "# --- Triton Benchmark ---\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['N'],  # argument names to use as an x-axis for the plot\n",
    "        # x_vals=[128 * i for i in range(55, 85)],  # different possible values for `x_name`\n",
    "        x_vals=[256 * i for i in range(30, 44, 2)],  # different possible values for `x_name`\n",
    "        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n",
    "        line_vals=['helion','triton', 'torch'],  # possible values for `line_arg`\n",
    "        line_names=[\"Helion two-pass Softmax\",\"Triton Fused-Softmax\", \"Torch Softmax\"],  # label name for the lines\n",
    "        styles=[('red', 'solid'),('cyan', 'solid'), ('orange', 'dashdot')],  # line styles\n",
    "        ylabel=\"GB/s\",  # label name for the y-axis\n",
    "        plot_name=\"Softmax Performance Benchamrk\",  # name for the plot. Used also as a file name for saving the plot.\n",
    "        args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`\n",
    "    ))\n",
    "def benchmark(M, N, provider):\n",
    "    # x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)\n",
    "    gen = torch.Generator(device=DEVICE).manual_seed(SEED)\n",
    "    x = torch.randn(M, N, device=DEVICE, dtype=torch.float32, generator=gen)\n",
    "    \n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    \n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: torch.softmax(x, dim=-1), rep=10, quantiles=quantiles\n",
    "        )\n",
    "    elif provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: softmax_triton(x), rep=10, quantiles=quantiles\n",
    "        )\n",
    "    elif provider == 'helion':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: softmax_helion(x), rep=10, quantiles=quantiles\n",
    "        )\n",
    "    # Calculate bandwidth: 2 * (read + write) * size / time\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms)\n",
    "\n",
    "# --- Run benchmark ---\n",
    "benchmark.run(show_plots=True, print_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f798046",
   "metadata": {},
   "source": [
    "\n",
    "## Summary \n",
    "Helion simplifies high-performance GPU kernel development. Through this workshop, developer has already know how to develop and optimize Helion kernel on AMD GPUs. \n",
    "\n",
    "We hope that this workshop will encourage you to tune, test, and contribute to Helion on AMD GPUs, and help us shape the future of AI acceleration.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6141c19b-08da-4def-8e63-781c55b9a14e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4912611-bfdd-433c-b559-81bbaab0d887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
